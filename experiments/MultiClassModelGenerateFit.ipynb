{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1db127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import load\n",
    "from torch.nn import functional as F\n",
    "from torch import autograd\n",
    "\n",
    "from torchvision import datasets\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "#from numba import njit\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "import gc\n",
    "\n",
    "import torchntk.autograd as ezntk\n",
    "\n",
    "import scipy.ndimage as ndimage\n",
    "\n",
    "from cleverhans.torch.attacks.fast_gradient_method import fast_gradient_method\n",
    "#from cleverhans.torch.attacks.projected_gradient_descent import (\n",
    "#    projected_gradient_descent,\n",
    "#)\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "from utils2 import process_Cifar10\n",
    "from einops import rearrange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a6bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, combined = process_Cifar10('./')\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2dbfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "__all__ = [\n",
    "    \"ResNet\",\n",
    "    \"resnet18\",\n",
    "    \"resnet34\",\n",
    "    \"resnet50\",\n",
    "]\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes,\n",
    "        planes,\n",
    "        stride=1,\n",
    "        downsample=None,\n",
    "        groups=1,\n",
    "        base_width=64,\n",
    "        dilation=1,\n",
    "        norm_layer=None,\n",
    "    ):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes,\n",
    "        planes,\n",
    "        stride=1,\n",
    "        downsample=None,\n",
    "        groups=1,\n",
    "        base_width=64,\n",
    "        dilation=1,\n",
    "        norm_layer=None,\n",
    "    ):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block,\n",
    "        layers,\n",
    "        num_classes=10,\n",
    "        zero_init_residual=False,\n",
    "        groups=1,\n",
    "        width_per_group=64,\n",
    "        replace_stride_with_dilation=None,\n",
    "        norm_layer=None,\n",
    "    ):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation)\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "\n",
    "        # CIFAR10: kernel_size 7 -> 3, stride 2 -> 1, padding 3->1\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        # END\n",
    "\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(\n",
    "            block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0]\n",
    "        )\n",
    "        self.layer3 = self._make_layer(\n",
    "            block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1]\n",
    "        )\n",
    "        self.layer4 = self._make_layer(\n",
    "            block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2]\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes,\n",
    "                planes,\n",
    "                stride,\n",
    "                downsample,\n",
    "                self.groups,\n",
    "                self.base_width,\n",
    "                previous_dilation,\n",
    "                norm_layer,\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        z = x.reshape(x.size(0), -1)\n",
    "        x = self.fc(z)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def forward_cam_conv1(self, x):\n",
    "        y = self.conv1(x)\n",
    "        x = self.bn1(y)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        z = x.reshape(x.size(0), -1)\n",
    "        x = self.fc(z)\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "    def forward_conv1(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return x\n",
    "    def forward_bn1(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        return x\n",
    "    def forward_layer1(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        return x\n",
    "    def forward_layer2(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "    def forward_layer3(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "    def forward_layer4(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "    def forward_flat(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        return x\n",
    "        \n",
    "\n",
    "\n",
    "def _resnet(arch, block, layers, pretrained, progress, device, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        script_dir = os.path.dirname(__file__)\n",
    "        state_dict = torch.load(\n",
    "            script_dir + \"/state_dicts/\" + arch + \".pt\", map_location=device\n",
    "        )\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(pretrained=False, progress=True, device=\"cpu\", **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet(\n",
    "        \"resnet18\", BasicBlock, [2, 2, 2, 2], pretrained, progress, device, **kwargs\n",
    "    )\n",
    "\n",
    "def resnet34(pretrained=False, progress=True, device=\"cpu\", **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet(\n",
    "        \"resnet34\", BasicBlock, [3, 4, 6, 3], pretrained, progress, device, **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def resnet50(pretrained=False, progress=True, device=\"cpu\", **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet(\n",
    "        \"resnet50\", Bottleneck, [3, 4, 6, 3], pretrained, progress, device, **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b5de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(\n",
    "                in_planes,\n",
    "                out_planes,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                groups=groups,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.ReLU6(inplace=True),\n",
    "        )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(round(inp * expand_ratio))\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            # pw\n",
    "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n",
    "        layers.extend(\n",
    "            [\n",
    "                # dw\n",
    "                ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            ]\n",
    "        )\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=10, width_mult=1.0):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "\n",
    "        # CIFAR10\n",
    "        inverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 1],  # Stride 2 -> 1 for CIFAR-10\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "        # END\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = int(input_channel * width_mult)\n",
    "        self.last_channel = int(last_channel * max(1.0, width_mult))\n",
    "\n",
    "        # CIFAR10: stride 2 -> 1\n",
    "        features = [ConvBNReLU(3, input_channel, stride=1)]\n",
    "        # END\n",
    "\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = int(c * width_mult)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                features.append(\n",
    "                    block(input_channel, output_channel, stride, expand_ratio=t)\n",
    "                )\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        z = x.mean([2, 3])\n",
    "        x = self.classifier(z)\n",
    "        return x\n",
    "\n",
    "\n",
    "def mobilenet_v2(pretrained=False, progress=True, device=\"cpu\", **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNetV2 architecture from\n",
    "    `\"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" <https://arxiv.org/abs/1801.04381>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(**kwargs)\n",
    "    if pretrained:\n",
    "        script_dir = os.path.dirname(__file__)\n",
    "        state_dict = torch.load(\n",
    "            script_dir + \"/state_dicts/mobilenet_v2.pt\", map_location=device\n",
    "        )\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c981d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, features, num_classes=10, init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        # CIFAR 10 (7, 7) to (1, 1)\n",
    "        # self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 1 * 1, 4096),\n",
    "            # nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "        self.fc = nn.Linear(4096, num_classes),\n",
    "        \n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        z = x.view(x.size(0), -1)\n",
    "        x = self.classifier(z)\n",
    "        return x, z\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == \"M\":\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfgs = {\n",
    "    \"A\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
    "    \"B\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
    "    \"D\": [\n",
    "        64,\n",
    "        64,\n",
    "        \"M\",\n",
    "        128,\n",
    "        128,\n",
    "        \"M\",\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "    ],\n",
    "    \"E\": [\n",
    "        64,\n",
    "        64,\n",
    "        \"M\",\n",
    "        128,\n",
    "        128,\n",
    "        \"M\",\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def _vgg(arch, cfg, batch_norm, pretrained, progress, device, **kwargs):\n",
    "    if pretrained:\n",
    "        kwargs[\"init_weights\"] = False\n",
    "    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)\n",
    "    if pretrained:\n",
    "        script_dir = os.path.dirname(__file__)\n",
    "        state_dict = torch.load(\n",
    "            script_dir + \"/state_dicts/\" + arch + \".pt\", map_location=device\n",
    "        )\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg11_bn(pretrained=False, progress=True, device=\"cpu\", **kwargs):\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg(\"vgg11_bn\", \"A\", True, pretrained, progress, device, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b643c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELNAME = 'ResNet18'\n",
    "modelname = 'resnet18'\n",
    "\n",
    "#modelname = 'mobilenet_v2'\n",
    "#modelname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a0cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODELNAME == 'ResNet18':\n",
    "    model = resnet18(device='cuda')\n",
    "    model.load_state_dict(torch.load('./resnet18.pt'))\n",
    "elif MODELNAME == 'ResNet34':\n",
    "    model = resnet34(device='cuda')\n",
    "    model.load_state_dict(torch.load('./resnet34.pt'))\n",
    "elif MODELNAME == 'MobileNetV2':\n",
    "    model = mobilenet_v2(device='cuda')\n",
    "    model.load_state_dict(torch.load('./mobilenet_v2.pt'))\n",
    "else:\n",
    "    raise ValueError('not a selected model')\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73260f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.CIFAR10(\n",
    "    root = '../DATA/',\n",
    "    train = True,                          \n",
    "    download = False,            \n",
    ")\n",
    "\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root = '../DATA/', \n",
    "    train = False, \n",
    "    download=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd4587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.tensor(train_data.data)\n",
    "test_x = torch.tensor(test_data.data)\n",
    "\n",
    "train_y = torch.tensor(train_data.targets)\n",
    "test_y = torch.tensor(test_data.targets)\n",
    "\n",
    "train_x_plot = torch.tensor(train_data.data).cpu().numpy()\n",
    "test_x_plot = torch.tensor(test_data.data).cpu().numpy()\n",
    "\n",
    "train_y_plot = torch.tensor(train_data.targets).cpu().numpy()\n",
    "test_y_plot = torch.tensor(test_data.targets).cpu().numpy()\n",
    "\n",
    "train_x = train_x/255\n",
    "test_x = test_x/255\n",
    "\n",
    "mean = torch.tensor([0.4914, 0.4822, 0.4465])\n",
    "std = torch.tensor([0.2471, 0.2435, 0.2616])\n",
    "\n",
    "test_x -= mean[None,None,None,:]\n",
    "test_x /= std[None,None,None,:]\n",
    "\n",
    "train_x -= mean[None,None,None,:]\n",
    "train_x /= std[None,None,None,:]\n",
    "\n",
    "train_y = train_y.float()\n",
    "test_y = test_y.float()\n",
    "\n",
    "train_x = train_x.to('cuda')\n",
    "train_y = train_y.to('cuda')\n",
    "\n",
    "test_x = test_x.to('cuda')\n",
    "test_y = test_y.to('cuda')\n",
    "\n",
    "train_x = rearrange(train_x,'b w h f -> b f w h')\n",
    "test_x = rearrange(test_x,'b w h f -> b f w h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d49213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_loader = DataLoader(train,batch_size=64,shuffle=False)\n",
    "test_loader = DataLoader(test,batch_size=64,shuffle=False)\n",
    "test_loader2 = DataLoader(test,batch_size=150,shuffle=False)\n",
    "combined_loader = DataLoader(combined,batch_size=150,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1429aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "model.to('cuda')\n",
    "correct=torch.tensor([0,],device='cuda')\n",
    "train_predictions = []\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(train_loader):\n",
    "        x, y = data\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        #out, z = model(x)\n",
    "        out = model(x)\n",
    "        classify = torch.argmax(torch.softmax(out,dim=1),dim=1)\n",
    "        num = torch.sum(classify==y)\n",
    "        correct+=num\n",
    "        train_predictions.append(classify.cpu().numpy())     \n",
    "print('NN Train acc: {:.2f}'.format((100*correct.cpu().numpy()/len(train_y))[0]))\n",
    "\n",
    "train_predictions=np.concatenate(train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d410ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "correct=torch.tensor([0,],device='cuda')\n",
    "predictions_model = []\n",
    "confidences_model_right_class = []\n",
    "confidences_model_guessed_class = []\n",
    "confidences_all = []\n",
    "NN_out = []\n",
    "NN_logits = []\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(test_loader):\n",
    "        x, y = data\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        out = model(x)\n",
    "        NN_logits.append(out.cpu().numpy())\n",
    "        classify = torch.argmax(torch.softmax(out,dim=1),dim=1)\n",
    "        NN_out.append(torch.softmax(out,dim=1))\n",
    "        for i in range(len(y)):\n",
    "            confidences_model_right_class.append(torch.softmax(out,dim=1)[i][torch.tensor(y[i],dtype=torch.long)])\n",
    "            confidences_model_guessed_class.append(torch.softmax(out,dim=1)[i][classify[i]])\n",
    "        confidences_all.append(out)\n",
    "        predictions_model.append(classify)\n",
    "        num = torch.sum(classify==y)\n",
    "        correct+=num\n",
    "print('NN Test acc: {:.2f}'.format((100*correct.cpu().numpy()/len(test_y))[0]))\n",
    "predictions_model = torch.cat(predictions_model)\n",
    "confidences_model_right_class = torch.stack(confidences_model_right_class).cpu().numpy()\n",
    "confidences_model_guessed_class = torch.stack(confidences_model_guessed_class).cpu().numpy()\n",
    "confidences_all = torch.cat(confidences_all).cpu().numpy()\n",
    "NN_out = torch.cat(NN_out).cpu().numpy()\n",
    "test_predictions = np.argmax(NN_out,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2ef6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_logits = np.concatenate(NN_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888feaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.special import erf\n",
    "from scipy.stats import norm\n",
    "from math import log\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b972fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_regression = train_y.cpu().numpy()\n",
    "validation_labels = test_y.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65b97c5",
   "metadata": {},
   "source": [
    "# load NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae88b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_test=10_000\n",
    "if modelname == 'resnet18':\n",
    "    NTK = torch.load(f'../KERNELS/CV/fullNTK.pt',map_location='cpu')\n",
    "    K0 = NTK[index_test::,index_test::].cpu().numpy()\n",
    "    K1 = NTK[0:index_test,index_test::].cpu().numpy()\n",
    "    K2 = NTK[0:index_test,0:index_test].cpu().numpy()\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "    #not included in this code release\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "K1 = K1 / (np.sqrt(np.diagonal(K0))[None,:]) / (np.sqrt(np.diagonal(K2))[:,None])\n",
    "K0 = K0 / np.sqrt(np.diagonal(K0))[:,None] / np.sqrt(np.diagonal(K0))[None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694579e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogKernelRegression_pNTK = SGDClassifier(loss='log_loss',penalty='l2',alpha=1e-4,fit_intercept=True,class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ec4ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_scale(LogKernelRegression,K0,K1,validation_labels):\n",
    "    LogKernelRegression.fit(K0,train_labels_regression)\n",
    "    weights = LogKernelRegression.coef_\n",
    "    intercept = LogKernelRegression.intercept_\n",
    "    y = LogKernelRegression.predict_proba(K1)\n",
    "    print((LogKernelRegression.predict(K1)==validation_labels.cpu().numpy()[0:len(y)]).sum()/len(y))\n",
    "    return weights, intercept, y, LogKernelRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a393353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tau_comparison_final(X,Y):    \n",
    "    \n",
    "    correct_logit_mask = test_y.cpu().numpy().astype(int)\n",
    "    \n",
    "    \n",
    "    X = np.array([softmax_numpy(X[i])[correct_logit_mask[i]] for i in range(len(X))])\n",
    "    Y = np.array([softmax_numpy(Y[i])[correct_logit_mask[i]] for i in range(len(Y))])    \n",
    "\n",
    "    \n",
    "    mask = np.logical_or(np.logical_or(X==1,Y==1),np.logical_or(X==0,Y==0))\n",
    "    X = X[~mask]\n",
    "    Y = Y[~mask]\n",
    "    \n",
    "    plt.plot(X,Y,'.')\n",
    "    \n",
    "    Tau = kendalltau(X,Y).correlation\n",
    "    return Tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a69e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K1_weights, K1_intercept, y_logisticregression, LogKernelRegression_pNTK = fit_and_scale(LogKernelRegression_pNTK,K0,K1,test_y)\n",
    "\n",
    "\n",
    "if modelname == 'resnet18':\n",
    "    LogKernelRegression_pNTK = load('../kGLMs/CV/pNTK.joblib')\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "    #not included in the code release\n",
    "    #LogKernelRegression_pNTK = load(f'XXX/large_model_NTKs/{modelname}/NTK_GLM.joblib')\n",
    "\n",
    "K1_weights = LogKernelRegression_pNTK.coef_\n",
    "K1_intercept = LogKernelRegression_pNTK.intercept_\n",
    "\n",
    "K1_scaled = (K1 @ K1_weights.T) + K1_intercept\n",
    "\n",
    "# test acc: ResNet18: 0.929\n",
    "# test acc: ResNet34: 0.9283\n",
    "# test acc: MobileNetV2: 0.9369"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a4e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_numpy(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "tau_comparison_final(K1_scaled,confidences_all)\n",
    "\n",
    "#ResNet18 Tau: 0.7755961255438976\n",
    "#ResNet34 Tau: 0.7858821360816239\n",
    "#MobileNetV2 Tau: 0.7003158758871003\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9448ccc7",
   "metadata": {},
   "source": [
    "# Using fit, now visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ac95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.special import erf\n",
    "from scipy.stats import norm\n",
    "from math import log\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def numpy_softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x),axis=1)[:,None]\n",
    "\n",
    "def sigmoid(p):\n",
    "    return 1/(1+np.exp(-p))\n",
    "\n",
    "def logit(p,eps=0.0):\n",
    "    return np.log((p+eps)/(1-p+eps))\n",
    "\n",
    "def softmax_numpy(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a976d3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_func(X,Theta):\n",
    "    \n",
    "    a=Theta[0]\n",
    "    b=Theta[1]\n",
    "    c=Theta[2]\n",
    "    s=Theta[3]\n",
    "    \n",
    "    Y = np.exp((X-a)/b)\n",
    "    Y = s*(Y/(1+Y)) + c\n",
    "    return Y\n",
    "\n",
    "def guesses(X,Y):\n",
    "    c = np.quantile(Y,0.05)\n",
    "    s = np.quantile(Y,0.90)-c\n",
    "    DB = (0.5*s + c)\n",
    "    a = 1*X[np.argmin(abs(Y-DB))]\n",
    "    \n",
    "    m_index = np.argsort(abs(Y-DB))[0:2]\n",
    "    m = (Y[m_index[1]] - Y[m_index[0]]) / (X[m_index[1]] - X[m_index[0]])\n",
    "    if m < 0:\n",
    "        m=m*-1\n",
    "    m=0.5\n",
    "\n",
    "    b = s/(4*m)\n",
    "    Theta = np.array([a,b,c,s])\n",
    "    #print('initial Guess: ',Theta)\n",
    "    return Theta\n",
    "\n",
    "def new_loss(Theta,X,Y):\n",
    "    delta=1.0\n",
    "    Yhat = new_func(X,Theta)\n",
    "    E=abs(Yhat-Y)\n",
    "    maskh = E>delta\n",
    "    HuberLoss=0\n",
    "    HuberLoss+= np.sum((delta*E[maskh] - 0.5*delta))\n",
    "    HuberLoss+= np.sum((delta*(Yhat-Y)[~maskh]**2))\n",
    "    return HuberLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d796b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import kdeplot\n",
    "def forward_fit_final(X,Y,method='Nelder-Mead',PLOT=True):\n",
    "    #X = kGLM activations\n",
    "    #Y = NN softmax vector\n",
    "    mask_trueclass = []\n",
    "    all_Xs = []\n",
    "    all_Ys = []\n",
    "    for class_ in range(10):\n",
    "        downselect_mask =  test_y.cpu().numpy().astype(int) == class_\n",
    "    \n",
    "        X1 = X[downselect_mask]\n",
    "        Y1 = Y[downselect_mask]\n",
    "    \n",
    "        for i in range(10):\n",
    "            \n",
    "            if i == class_:\n",
    "                if class_!=0:\n",
    "                    where = np.array([class_,class_-1],dtype=int)\n",
    "                else:\n",
    "                    where = np.array([class_,1],dtype=int)\n",
    "            if i != class_:\n",
    "                where = np.array([class_,i],dtype=int)\n",
    "\n",
    "            X2=X1[:,where]\n",
    "            how_many=len(X2)\n",
    "            X2 = X2.flatten()\n",
    "            Y2=Y1[:,where].flatten()\n",
    "        \n",
    "            x0 =  guesses(X2,Y2)\n",
    "            res = minimize(new_loss,x0=x0,args=(X2,Y2),method=method)\n",
    "            Theta = res.x\n",
    "            DS = 0.5*Theta[3] +Theta[2]\n",
    "            Yhat = new_func(X2,Theta)\n",
    "\n",
    "            if i != class_:\n",
    "                all_Xs.append(Yhat[1::2])\n",
    "                all_Ys.append(Y2[1::2])\n",
    "                mask_trueclass.append(np.zeros(len(Y2[1::2]),dtype=bool))\n",
    "            if i == class_:\n",
    "                all_Xs.append(Yhat[0::2])\n",
    "                all_Ys.append(Y2[0::2])\n",
    "                mask_trueclass.append(np.ones(len(Y2[0::2]),dtype=bool))\n",
    "        \n",
    "    mask_trueclass = np.concatenate(mask_trueclass)\n",
    "#     for i in range(len(mask_trueclass)):\n",
    "#         if mask_trueclass[i]:\n",
    "#             continue\n",
    "#         else:\n",
    "#             if np.random.random() > 0.9:\n",
    "#                 mask_trueclass[i] = True\n",
    "    all_Xs = logit(np.array(all_Xs).flatten())\n",
    "    all_Ys = logit(np.array(all_Ys).flatten())\n",
    "    print(all_Xs.shape)\n",
    "    print(all_Ys.shape)\n",
    "    \n",
    "    maskout = np.logical_or(np.logical_or(np.isnan(all_Xs),np.isinf(all_Xs)),np.logical_or(np.isnan(all_Ys),np.isinf(all_Ys)))\n",
    "    all_Xs = all_Xs[~maskout]\n",
    "    all_Ys = all_Ys[~maskout]\n",
    "    mask_trueclass = mask_trueclass[~maskout]\n",
    "    R_squared = r2_score(all_Xs,all_Ys,)\n",
    "    center_mask1 = np.logical_and(all_Xs[mask_trueclass]>-5,all_Xs[mask_trueclass]<-0)\n",
    "    center_mask2 = np.logical_and(all_Xs[~mask_trueclass]>-5,all_Xs[~mask_trueclass]<-0)\n",
    "    if PLOT:\n",
    "        #After Mapping\n",
    "#         plt.plot(all_Xs[~mask_trueclass],all_Ys[~mask_trueclass],'.',alpha=0.05,)\n",
    "#         plt.plot(all_Xs[mask_trueclass],all_Ys[mask_trueclass],'s',alpha=0.05,)\n",
    "#         plt.plot([np.min(all_Ys),np.max(all_Ys)],[np.min(all_Ys),np.max(all_Ys)],'k--',alpha=1.0,label='parity')\n",
    "#         plt.ylabel(f'All logits NN ',fontsize=14)\n",
    "#         plt.xlabel(f'All logits kGLM',fontsize=14)\n",
    "#         plt.title(f'ResNet18 Linearization',fontsize=14)\n",
    "#         plt.ylim(np.min(all_Ys),np.max(all_Ys)+1)\n",
    "#         plt.xlim(np.min(all_Xs),np.max(all_Xs)+1)\n",
    "#         plt.plot(-1000,-1000,'.',alpha=1.0,color='tab:blue',label='Incorrect Class')\n",
    "#         plt.plot(-1000,-1000,'.',alpha=1.0,color='tab:orange',label='Correct Class')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        #plt.plot(all_Xs[~mask_trueclass][center_mask2],all_Ys[~mask_trueclass][center_mask2],'.',alpha=0.05,color='tab:blue')\n",
    "        #plt.plot(all_Xs[mask_trueclass][center_mask1],all_Ys[mask_trueclass][center_mask1],'s',alpha=0.05,color='tab:orange')\n",
    "        kdeplot(all_Xs[~mask_trueclass][::1000],all_Ys[~mask_trueclass][::1000],fill=True,color='tab:blue')\n",
    "        kdeplot(all_Xs[mask_trueclass][::100],all_Ys[mask_trueclass][::100],fill=True,color='tab:orange')\n",
    "        \n",
    "        plt.plot([np.min(all_Ys),np.max(all_Ys)],[np.min(all_Ys),np.max(all_Ys)],'r--',alpha=1.0,label='parity')\n",
    "        plt.ylabel(f'All logits NN ',fontsize=14)\n",
    "        plt.xlabel(f'All logits kGLM',fontsize=14)\n",
    "        plt.title(f'{MODELNAME} Linearization',fontsize=14)\n",
    "        plt.ylim(-12,8)\n",
    "        plt.xlim(-12,8)\n",
    "        plt.plot(-1000,-1000,color='tab:blue',label='Incorrect Class Logit')\n",
    "        plt.plot(-1000,-1000,color='tab:orange',label='Correct Class Logit')\n",
    "        plt.text(-5,2,'R^2 = {:.3f}'.format(R_squared),fontsize=12)\n",
    "        plt.legend()\n",
    "       \n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    \n",
    "    return R_squared#, all_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d63e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_fit_final(K1_scaled,numpy_softmax(confidences_all),)\n",
    "\n",
    "\n",
    "#R2 ResNet18: 0.9139234434009396\n",
    "#R2 ResNet34: 0.9202184987953692\n",
    "#R2 MobileNetV2: 0.9119214895054525"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
