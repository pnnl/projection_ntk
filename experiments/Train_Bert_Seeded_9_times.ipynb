{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3eaeae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import load\n",
    "from torch.nn import functional as F\n",
    "from torch import autograd\n",
    "\n",
    "from torchvision import datasets\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from numba import njit\n",
    "\n",
    "import os\n",
    "\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from easyntk.explicit import explicit_ntk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b37d9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# because the dataset is int tsv format we have to use delimeter.\n",
    "df = pd.read_csv(\"../DATA/cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_sources', 'label', 'label_note', 'sentence'])\n",
    "\n",
    "# creating a copy so we don't messed up our original dataset.\n",
    "data=df.copy()\n",
    "\n",
    "data.drop(['sentence_sources','label_note'],axis=1,inplace=True)\n",
    "sentences=data.sentence.values\n",
    "labels = data.label.values\n",
    "data.head()\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "# using the low level BERT for our task.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Printing the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Printing the tokenized sentence in form of list.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))\n",
    "\n",
    "input_ids = []\n",
    "for sent in sentences:\n",
    "    # so basically encode tokenizing , mapping sentences to thier token ids after adding special tokens.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence which are encoding.\n",
    "                        add_special_tokens = True, # Adding special tokens '[CLS]' and '[SEP]'\n",
    "\n",
    "                         )\n",
    "    \n",
    " \n",
    "    input_ids.append(encoded_sent)\n",
    "    \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN , truncating=\"post\", padding=\"post\")\n",
    "\n",
    "attention_masks = []\n",
    "\n",
    "for sent in input_ids:\n",
    "    \n",
    "    # Generating attention mask for sentences.\n",
    "    #   - when there is 0 present as token id we are going to set mask as 0.\n",
    "    #   - we are going to set mask 1 for all non-zero positive input id.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "   \n",
    "    attention_masks.append(att_mask)\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, test_size=0.2, random_state=0)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,test_size=0.2, random_state=0)\n",
    "\n",
    "#changing the numpy arrays into tensors for working on GPU. \n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Deciding the batch size for training.\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "#DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = SequentialSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97638cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7157b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for SEED in range(3,11):\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels = 2,   \n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False,\n",
    "    )\n",
    "    \n",
    "    model.to('cuda')\n",
    "\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5,\n",
    "                  eps = 1e-8)\n",
    "\n",
    "    train_data = TensorDataset(train_inputs, train_labels, train_masks)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=64, shuffle=False)\n",
    "\n",
    "    total_loss = 0\n",
    "        # putting model in traing mode there are two model eval and train for model\n",
    "    model.train()\n",
    "    device='cuda'\n",
    "    for epoch in range(10):\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            #getting ids,mask,labes for every batch\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[2].to(device)\n",
    "            b_labels = batch[1].to(device)\n",
    "\n",
    "            outputs = model(b_input_ids,\n",
    "                   token_type_ids=None,\n",
    "                   attention_mask=b_input_mask,\n",
    "                   labels=b_labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    torch.save(model.state_dict(),'./MANY_BERT_MODELS/BERT-base_SEED{}.pt'.format(SEED))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
